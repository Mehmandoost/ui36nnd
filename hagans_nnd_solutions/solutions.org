
#+TITLE: Neural Network Design Solutions
#+AUTHOR: Ali Mehmandoost
#+DATE: 
#+EMAIL: mehmandoost@eng.ui.ac.ir
#+LaTeX: \setcounter{secnumdepth}{0}

\newpage
The following text is not a solutions manual nor it was intended to be any kind of guid. It's just My HomeWork or pre-exame practice. It is incomplete and full of errors.
If you find an error you can inform me via github issues [https://github.com/Mehmandoost/ui36nnd/issues].
\newpage
* Chapter 2
** E2.1
  weight = 1.3, bias = 3 \\
  f( Wp + b ) 
*** I:
**** Linear, Positive Linear:\\
\begin{equation}
\[ purelin(1.3*p+3) = 1.6 \]
\[ poslin(1.3*p+3) = 1.6 \]
\[1.3*p+3 = 1.6 \implies p = -1.076923077\]
\end{equation}
*** II:
**** Linear, Positive Linear:\\
\begin{equation}
\[ purelin(1.3*p+3) = 1 \]
\[ poslin(1.3*p+3) = 1 \]
\[1.3*p+3 = 1 \implies p = -1.538461538\]
\end{equation}
**** Hard Limit, Symmetric Hard Limit:\\
\begin{equation}
\[harlim(1.3*p+3) = 0\]
\[harlims(1.3*p+3) = 0\]
\[1.3*p+3>0 \implies p > -2.307692308 \]
\end{equation}

**** Saturating Linear, Symmetric Saturating Linear:\\
\begin{equation}
\[satlin(1.3*p+3) = 1\]
\[satlins(1.3*p+3) = 1\]
\[(1.3*p+3)>1 \implies p > -1.538461538\]
\end{equation}

*** III:
****  Linear, Positive Linear, Saturating Linear, Symmetric Saturating Linear:\\
\begin{equation}
\[ purelin(1.3*p+3) = 0.9963 \]
\[ poslin(1.3*p+3) = 0.9963 \]
\[ satlin(1.3*p+3) = 0.9963 \]
\[ stalins(1.3*p+3) = 0.9963 \]
\[1.3*p+3 = 1.6 \implies p = -1.541307692\]
\end{equation}
**** Log-Sigmoid:\\
\begin{equation}
\[ logsig(1.3*p+3) = 0.9963 \]
\[\frac{1}{1+e^{-(1.3*p+3)}} = 0.9963 \implies 1+e^{-(1.3*p+3)} = 1.003713741 \]
\[ \implies e^{-(1.3*p+3)} = 0.003713741 \implies -(1.3*p+3) = \ln0.003713741\]
\[\implies p = 1.996704306\]
\end{equation}

**** Hyperbolic Tangent Sigmoid:\\
\begin{equation}
\[ tansig(1.3*p+3) = 0.9963 \]
\[\frac{e^{(1.3*p+3)}-e^{-(1.3*p+3)}}{e^{(1.3*p+3)}+e^{-(1.3*p+3)}} = 0.9963 \implies \]
\[0.0037e^{(1.3*p+3)}-1.9963e^{-(1.3*p+3)} = 0 \implies \ln0.0037e^{(1.3*p+3)} = \ln1.9963e^{-(1.3*p+3)} \implies \]
\[\\ln0.0037 + (1.3*p+3) = \ln1.9963 -(1.3*p+3) \implies \]
\[(1.3*p+3) = −2.454063496 \implies p = −4.195433459\]
\[ \implies e^{-(1.3*p+3)} = 0.003713741 \implies -(1.3*9+3) = \ln0.003713741 \]
\[\implies p = 1.996704306\]
\end{equation}
*** IV:
**** Symmetric Hard Limit:\\
\begin{equation}
\[ hardlim(1.3*p+3) = -1 \implies \]
\[ 1.3*p+3 < 0 \implies p < -2.307692308\]
\end{equation}
**** Linear:\\
\begin{equation}
\[ purelin(1.3*p+3) = -1 \implies \]
\[ 1.3*p+3 = -1 \implies p = -3.076923077\]
\end{equation}

**** Symmetric Saturating Linear:\\
\begin{equation}
\[ satlin(1.3*p+3) \leq -1 \implies 1.3*p+3 \leq -1 \implies p \leq -3.076923077 \]
\end{equation}

** E2.2
*** I:
    Symmetrical Hard Limit
*** II:
 -3*weight, yes bias is product of -3 and wieght.
*** III: @TODO

** E2.3
*** I:
no:
\begin{equation}
\[
\begin{bmatrix}
x_{3} & x_{2}
\end{bmatrix}
\begin{bmatrix}
x_{3}\\
 x_{2}
\end{bmatrix}
= -1
\]
\[hardlim(-1) = 0  \]
\[hardlims(-1) = -1 \]
\[purelin(-1) = -1 \]
\[satlin(-1) = 0 \]
\[satlins(-1) = -1 \]
\[logsig(-1) = 0.26894 \]
\[tansig(-1) = -0.76159 \]
\[poslin(-1) = 0\]
\end{equation}

*** II:
\begin{equation}
purelin(-1+b) = 0.5 \implies -1+b = 0.5 \implies b = 1.5
\end{equation}

*** III: 
\begin{equation}
\[logsig(-1+b) = 0.5 \implies \frac{1}{1+e^{-(-1+b)}} = 0.5 \implies e^{-(-1+b)} = 1 \implies\]
\[-1+b = 0 \implies b = 1\]
\end{equation}

*** IV:
No,  Symmetrical hard limit domain is {-1,1} therefore it can not produce 0.5.

** E2.4
*** I:
six neurons, one for each out put and atleast 1 multi input neuron for  inputs
*** II:
Not enough information is given but with minimal assumption of E2.4.I at input neuron 1 row for the neuron and 6 column for each input is requied and each out put neuron has 1\times1 wieght matrix.
*** III: 
Not enough information is given to determine the transfer function of the input layer but of the transfer functions from Table 2.1, the logsig transfer function would be most appropriate for output layer.
*** IV:
 Not enough information is given to determine if a bias is required.

** E2.5
*** I: 
[[./graphs/e2_5_I.png]]
*** II: 
[[./graphs/e2_5_II.png]]

*** III: 
[[./graphs/e2_5_III.png]]

*** IV: 
[[./graphs/e2_5_IV.png]]

*** V: 
[[./graphs/e2_5_V.png]]

** E2.6
*** I: 
[[./graphs/e2_6_I.png]]
*** II: 
[[./graphs/e2_6_II.png]]

*** III: 
[[./graphs/e2_6_III.png]]

*** IV: 
[[./graphs/e2_6_IV.png]]

*** V: 
[[./graphs/e2_6_V.png]]

*** VI: 
[[./graphs/e2_6_VI.png]]
* Chapter 3
** E3.1
*** I: 
\begin{equation}
\[
w = 
\begin{bmatrix}
0 & 0 & 1
\end{bmatrix}
 and b = 0
\end{equation}

*** II: 
**** Feed Forward Layer: 
\begin{equation}
\[
w^{1} = 
\begin{bmatrix}
P_{1}^{T} &P_{2}^{T}
\end{bmatrix}
= 
\begin{bmatrix}
-1   &1  &-1 \\
-1  &-1   &1
\end{bmatrix}
\\
\end{equation}

and bias is number of elements in input vector:
\begin{equation}
\[
b = 
\begin{bmatrix}
3 \\ 3
\end{bmatrix}

\end{equation}

**** Recurrent Layer:

\begin{equation}
\[
w^{2} = 
\begin{bmatrix}
1 &-\xi \\
-\xi &1
\end{bmatrix}
\]
\[ \xi < \frac{1}{S-1} \]
\end{equation}
 S is number of neurons in recurrent layer 
\begin{equation}
\[
\xi < 1 \implies \xi = 0.5 \\
\\
\implies w = 
\begin{bmatrix}
1 &-0.5 \\
-0.5 &1
\end{bmatrix}
\]
\end{equation}
*** III: 

\begin{equation}
\[
w = 
\begin{bmatrix}
1 &0 &0 \\
0 &1 &0 \\
0 &0 &1 \\
\end{bmatrix}
\\
\]
\end{equation}

\begin{equation}
\[
b = \begin{bmatrix}0 \\ 0 \\ 0\\ \end{bmatrix}
\]
\end{equation}

** E3.2
*** I: 
[[./graphs/e3_2_I.png]]

*** II: 
we pick two point: x_{1} = \begin{bmatrix}0\\2\end{bmatrix} and x_{2} \begin{bmatrix}2\\0\end{bmatrix} on decision boundary:
\begin{equation}
\[
\begin{bmatrix}w_{1} && w_{2}\end{bmatrix} \times \begin{bmatrix}0\\2\end{bmatrix}  + b = 0 
\]
\[
\begin{bmatrix}w_{1} && w_{2}\end{bmatrix} \times \begin{bmatrix}2\\0\end{bmatrix}  + b = 0 \\
\]
\implies \begin{array}{ll} 
2w_{1} + b = 0 \\
2w_{2} + b = 0
\end{array}
\implies
W^{T} = [-1 &-1] \ \& \ b = 2
\]
\end{equation}
*** III: 
\begin{equation}
\[
hardlim(W^{T}P+b) = 
hardlim( \begin{bmatrix}-1 &-1\end{bmatrix}\begin{bmatrix}1 \\ 0\end{bmatrix} + 2) = 1 
\]
\end{equation}

lets calculate distance of the input from each point:
\begin{equation}
\[
Distance\ from\ P_{1} = \sqrt{(1-1)^{2}+(0.5-0)^{2}} = 0.25 
\]
\[
Distance\ from\ P_{2} = \sqrt{(2-1)^{2}+(1-0)^{2}} \simeq 1.4142 \\
\]
\end{equation}
yes it choses the point with minimum distance to input.

*** IV: 
We can't, Hamming network was designed explicity to solve binary pattern recognition problems.
** E3.3
*** I
\begin{equation}
\[Satlins(\begin{bmatrix}1 &-1 \\ -1 &1\end{bmatrix} \times \begin{bmatrix}0.9\\1\end{bmatrix}) = \begin{bmatrix}-1\\1\end{bmatrix}\]
\[Satlins(\begin{bmatrix}1 &-1 \\ -1 &1\end{bmatrix} \times \begin{bmatrix}-1\\1\end{bmatrix}) = \begin{bmatrix}-0.2\\0.2\end{bmatrix}\]
\[Satlins(\begin{bmatrix}1 &-1 \\ -1 &1\end{bmatrix} \times \begin{bmatrix}-0.2\\0.2\end{bmatrix}) = \begin{bmatrix}-0.4\\0.4\end{bmatrix}\]
\[Satlins(\begin{bmatrix}1 &-1 \\ -1 &1\end{bmatrix} \times \begin{bmatrix}-0.4\\0.4\end{bmatrix}) = \begin{bmatrix}-0.8\\0.8\end{bmatrix}\]
\[Satlins(\begin{bmatrix}1 &-1 \\ -1 &1\end{bmatrix} \times \begin{bmatrix}-0.8\\0.8\end{bmatrix}) = \begin{bmatrix}-1\\1\end{bmatrix}\]
\[Satlins(\begin{bmatrix}1 &-1 \\ -1 &1\end{bmatrix} \times \begin{bmatrix}-1\\1\end{bmatrix}) = \begin{bmatrix}-1\\1\end{bmatrix}\]
\end{equation}

*** II: 
[#TODO]
*** III: 
[#TODO]

** E3.4
*** I, II: 
[#ASK: Should I use W^{T}]
3 different classes: it divides the space in to 3 parts 
\begin{equation}
\[ \begin{bmatrix}1 && 1\end{bmatrix} \begin{bmatrix}x && y\end{bmatrix}  - 2 = 0 \implies y = 2-x \]
\[ \begin{bmatrix}-1 && 1\end{bmatrix} \begin{bmatrix}x && y\end{bmatrix}  - 0 = 0 \implies y= x \]
\[ R1 = \begin{bmatrix}1 && 1\end{bmatrix} \]
\[ R2 = \begin{bmatrix}-1 && 1\end{bmatrix}\]
\[ R2 = \begin{bmatrix}-1 && 0\end{bmatrix}\]
\end{equation}
[[./graphs/e3_4_II.png]]

*** III: 
\begin{equation}
\[hardlims(\begin{bmatrix}1 &1 \\ -1 &1\end{bmatrix} \times \begin{bmatrix}1\\-1\end{bmatrix} + \begin{bmatrix}-2\\0\end{bmatrix})  = \begin{bmatrix}-1\\-1\end{bmatrix}\]
\end{equation}
*** IV: 
[[./graphs/e3_4_IV.png]]\\
the input is in decision boundary we can't choose which area is the right region.

** E3.5
*** I: 
[[./graphs/e3_5_I.png]]
*** II: 
we pick two points on decision boundary x_{1} =  \begin{bmatrix}0  \\ 1.5\end{bmatrix}  and x_{2} =  \begin{bmatrix}-1.5  \\ 0\end{bmatrix}
\begin{equation}
\[
\begin{bmatrix}w_{1} && w_{2}\end{bmatrix} \begin{bmatrix}0\\ 1.5\end{bmatrix}  + b = 0 
\]
\[
\begin{bmatrix}w_{1} && w_{2}\end{bmatrix} \begin{bmatrix}-1.5\\0\end{bmatrix}  + b = 0 \\
\]
\implies \begin{array}{ll} 
-1.5w_{1} + b = 0 \\
1.5w_{2} + b = 0
\end{array}
\implies
W^{T} = [1 &-1] \ \& \ b = 1.5
\]
\end{equation}

*** III: 
[[./graphs/e3_5_III.png]]

*** IV

\begin{equation}
\[hardlims(\begin{bmatrix}1 && -1\end{bmatrix} \begin{bmatrix}-1\\ 0\end{bmatrix}  + 1.5) = 1 \]
\[hardlims(\begin{bmatrix}1 && -1\end{bmatrix} \begin{bmatrix}1\\ 2\end{bmatrix}  + 1.5) = 1 \]
\[hardlims(\begin{bmatrix}1 && -1\end{bmatrix} \begin{bmatrix}-1\\ 1\end{bmatrix}  + 1.5) = -1 \]
\[hardlims(\begin{bmatrix}1 && -1\end{bmatrix} \begin{bmatrix}0\\ 2\end{bmatrix}  + 1.5) = -1 \]
\end{equation}
*** V:
Yes they are many lines that can divide the space between these two classes.\\
Yes my W and b  minimise sum of distance from the decision boundary.
[#Todo: #DoTheMath]

** E3.6
*** I: 
[[./graphs/e3_6_I.png]]
*** II: 

\begin{equation}
w = \begin{bmatrix}1 &0\end{bmatrix} \ \& \ b=0
\end{equation}

*** III: 
[[./graphs/e3_6_III.png]]

*** IV: 
\begin{equation}
\[n = Wp+b =  \begin{bmatrix}1 &0\end{bmatrix}  \begin{bmatrix}0.5 \\-0.5\end{bmatrix} + 0 = 0.5 \]
\[a= hardlims(Wp+b) =  hardlims(0.5) = 1 \]
\end{equation}
\\ yes it is closer to \begin{bmatrix}1 \\ 1\end{bmatrix} and the network classified them in the same class.
*** V: 
**** Feed forward Layer
\begin{equation}
\[W^{1} =  \begin{bmatrix}P_{1}^{T} \\ P_{2}^{T} \end{bmatrix} =  \begin{bmatrix}-1 &1 \\ 1 &1\end{bmatrix}\]
\[b^{1} = \begin{bmatrix}2 \\ 2 \end{bmatrix}\]
\[a^{1} = purelen(W^{1}p+b^{1}) \]
\end{equation}
**** Recurrent Layer
\begin{equation}
\[W^{2} =  \begin{bmatrix}1 &-0.5 \\ -0.5 &1 \end{bmatrix}\]
\[a^{2}(0) = a^{1} ,  a^{2}(t+1) = poslin(W^{2} a^{2}(t))\]
\end{equation}

**** VI: \\
\begin{equation}
\[a^{1} = purelen(\begin{bmatrix}-1 &1 \\ 1 &1\end{bmatrix} \begin{bmatrix}0.5 //- 0.5 \end{bmatrix} + \begin{bmatrix}2 \\ 2 \end{bmatrix}) = \begin{bmatrix}1 \\ 2 \end{bmatrix}\] \]
\[a^{2}(1) = a^{1} ,  a^{2}(t+1) = poslin( \begin{bmatrix}1 &-0.5 \\ -0.5 &1 \end{bmatrix} \begin{bmatrix}1 \\ 2 \end{bmatrix}) =  \begin{bmatrix}0 \\ 1.5 \end{bmatrix} \]
\end{equation}

**** VII: \\
[#TODO]
**** VIII: \\
[#TODO]
** E3.7
[#TODO]

* Chapter 4
** E4.1
*** I: 
[[./graphs/e4_1_I.png]]

*** II:
#+begin_src octave
plot(-1,1, 'xb;t=1;');
hold on;
plot(0,0, 'xb');
plot(1, -1, 'xb');
plot(1,0, 'or;t=0;');
plot(0,1, 'or');
x = [-2:0.01:2];
plot (x, 0.5-x, ";y=-x+0.5;");
grid on;
#+end_src

[[./graphs/e4_1_II.png]]

yes it divides the space in two different classes each contains points with the same target.
** E4.2
*** I:
#+begin_src octave
plot(-1,1, 'xb;t=1;');
hold on;
plot(-1,-1, 'xb');
plot(0,0, 'or;t=0;');
plot(1,0, 'or');
x(1:401) = -0.5;
y = [-2:0.01:2];
plot (x, y, ";x= 0.5;");
grid on;
#+end_src
[[./graphs/e4_2_I.png]]

\begin{equation}
\[P = \begin{bmatrix}-0.5 \\ 0\end{bmatrix}\]
\[W = \begin{bmatrix}-1 \\ 0\end{bmatrix}\]
\[W^{T}P+b = 0 \implies \begin{bmatrix}-1 &0\end{bmatrix}  \begin{bmatrix}-0.5 \\ 0\end{bmatrix} + b =  0.5 + b \implies b = -0.5\]
\end{equation}

*** II, III: 
#+begin_src octave
p_1 = [-1; 1];
p_2 = [-1; -1];
p_3 = [0; 0];
p_4 = [1; 0];
p_5 = [-2; 0];
p_6 = [1; 1];
p_7 = [0; 1];
p_8 = [-1; -2];
function a = hardlim(n)
a = n;
a(n<0) = 0;
a(n>0) = 1;
endfunction

function a = perseptron (p)
W = [-1, 0];
b = -0.5;
a = hardlim(W*p+b);
endfunction

[perseptron(p_1), perseptron(p_2), perseptron(p_3), perseptron(p_4), ...
 perseptron(p_5), perseptron(p_6), perseptron(p_7), perseptron(p_8)]
#+end_src

ans =

   1   1   0   0   1   0   0   1


*** IV: 
#+begin_src octave
plot(-2,0, 'xb');
hold on;
plot(1,1, 'xb');
plot(0,1, 'xb');
plot(-1,-2, 'xb');
x(1:401) = -0.5;
y = [-3:0.01:3];
grid on;
#+end_src
[[./graphs/e4_2_I.png]]
*** V: 
[#ASK] 
All the vectors may result in a different classes simply by changing W to its reflection in respect to decision boundary.
** E4.3

\begin{equation}
\[ P_{3} = \begin{bmatrix}0 \\ 0\end{bmatrix} \implies b < 0 \implies b = -1\]
\[ P_{4} = \begin{bmatrix} 1 \\ 0\end{bmatrix} \implies w_{1} + b < 0 \implies w_{1} = -2\]
\[ P_{2} = \begin{bmatrix}-1 \\ -1\end{bmatrix} \implies -w_{1}+w_{2} +b > 0 \implies 2 + w_{2} -1 > 0 \implies w_{2} > -1  \]
\[P = \begin{bmatrix}-1 \\ -1\end{bmatrix} \implies -w_{1}-w_{2} +b > 0 \ implies 2 -w_{2} -1 > 0 \implies w_{2} <  1  \]
\[-1 <w_{2}< 1 \implies w_{2} = -0.5 \] 
\end{equation}

#+begin_src octave
p_1 = [-1; 1];
p_2 = [-1; -1];
p_3 = [0; 0];
p_4 = [1; 0];
p_5 = [-2; 0];
p_6 = [1; 1];
p_7 = [0; 1];
p_8 = [-1; -2];
function a = hardlim(n)
a = n;
a(n<0) = 0;
a(n>0) = 1;
endfunction

function a = perseptron (p)
W = [-2, 0];
b = -0.5;
a = hardlim(W*p+b);
endfunction

[perseptron(p_1), perseptron(p_2), perseptron(p_3), perseptron(p_4), ...
 perseptron(p_5), perseptron(p_6), perseptron(p_7), perseptron(p_8)]
#+end_src

ans =

   1   1   0   0   1   0   0   1

** E4.4
\begin{equation}
\[ W(0) = \begin{bmatrix}0, 0\end{bmatrix},  b(0)= 0\]
\[hardlim(W*p_{1}+b) = hardlim(\begin{bmatrix}0, 0\end{bmatrix} \begin{bmatrix}-1 \\ 1\end{bmatrix} + 0) = 1 \implies e = 1 - 1 = 0 \implies W(1) = W(0), b(1) = b(0) \]
\[hardlim(W*p_{2}+b) = hardlim(\begin{bmatrix}0, 0\end{bmatrix} \begin{bmatrix}-1 \\ -1\end{bmatrix} + 0) = 1 \implies e = 1 - 1 = 0 \implies W(2) = W(1) ,b(2) = b(1)\]
\[hardlim(W*p_{3}+b) = hardlim(\begin{bmatrix}0, 0\end{bmatrix} \begin{bmatrix}0 \\ 0\end{bmatrix} + 0) = 1 \implies e = 0 -1 = -1 \implies\]
\[ W(3) = W(2) - p_{3}^{t} = \begin{bmatrix}0, 0\end{bmatrix} - \begin{bmatrix}0 &0\end{bmatrix} = \begin{bmatrix}0 \\ 0\end{bmatrix} , b (3) = b(2) + e = 0-1 = -1\]
\[hardlim(W*p_{4}+b) = hardlim(\begin{bmatrix}0, 0\end{bmatrix} \begin{bmatrix}1 \\ 0\end{bmatrix} - 1 ) = 0 \implies e = 0 -0 = -0 \implies W(4) = W(3), b(4) = b(3)\]
\[hardlim(W*p_{1}+b) = hardlim(\begin{bmatrix}0, 0\end{bmatrix} \begin{bmatrix}-1 \\ 1\end{bmatrix} - 1 ) = 0 \implies e = 1 -0 = 1 \implies \]
\[ W(5) = W(4) + p_{1}^{T} = \begin{bmatrix}0, 0\end{bmatrix} + \begin{bmatrix}-1 &1\end{bmatrix} = \begin{bmatrix}-1 \\ 1\end{bmatrix} , b (5) = b(4) + e = -1 + 1  = 0\]
\[hardlim(W*p_{2}+b) = hardlim(\begin{bmatrix}-1, 1\end{bmatrix} \begin{bmatrix}-1 \\ -1\end{bmatrix} +0  ) = 0 \implies e = 1 - 0 = 1 \implies \]
\[ W(6) = W(5) + p_{2}^{T} = \begin{bmatrix}-1, 1\end{bmatrix} + \begin{bmatrix}-1 &-1\end{bmatrix} = \begin{bmatrix}-2 \\ 0\end{bmatrix} , b (6) = b(5) + e = 0 + 1  = 1\]
\[hardlim(W*p_{3}+b) = hardlim(\begin{bmatrix}-2, 0\end{bmatrix} \begin{bmatrix} 0 \\ 0 \end{bmatrix} +1  ) = 1 \implies e = 0 - 1 = -1 \implies \]
\[ W(7) = W(6) + p_{3}^{T} = \begin{bmatrix}-2 \\ 0\end{bmatrix} , b (7) = b(6) + e = 1 - 1  = 0\]
\[hardlim(W*p_{4}+b) = hardlim(\begin{bmatrix}-2, 0\end{bmatrix} \begin{bmatrix}1 \\ 0\end{bmatrix} - 0  ) = 0 \implies e = 0 - 0 = 0 \implies W(8) = W(7), b(8) = b(7) \]
\[hardlim(W*p_{1}+b) = hardlim(\begin{bmatrix}-2, 0\end{bmatrix} \begin{bmatrix}-1 \\ 1\end{bmatrix} - 0  ) = 1 \implies e = 1 - 1 = 0 \implies W(9) = W(8), b(9) = b(8) \]
\[hardlim(W*p_{2}+b) = hardlim(\begin{bmatrix}-2, 0\end{bmatrix} \begin{bmatrix}-1 \\ -1\end{bmatrix} - 0  ) = 1 \implies e = 1 - 1 = 0 \implies W(10) = W(9), b(10) = b(9) \]
\[hardlim(W*p_{3}+b) = hardlim(\begin{bmatrix}-2, 0\end{bmatrix} \begin{bmatrix}0 \\ 0\end{bmatrix} - 0  ) = 0 \implies e = 0 - 0 = 0 \implies W(11) = W(10), b(11) = b(10) \]
\[\]
\[W = \begin{bmatrix}-2, 0\end{bmatrix}, b = 0 \]
\end{equation}

#+begin_src octave
p_1 = [-1; 1];
p_2 = [-1; -1];
p_3 = [0; 0];
p_4 = [1; 0];
p_5 = [-2; 0];
p_6 = [1; 1];
p_7 = [0; 1];
p_8 = [-1; -2];
function a = hardlim(n)
a = n;
a(n<0) = 0;
a(n>0) = 1;
endfunction

function a = perseptron (p)
W = [-2, 0];
b = 0;
a = hardlim(W*p+b);
endfunction

[perseptron(p_1), perseptron(p_2), perseptron(p_3), perseptron(p_4), ...
 perseptron(p_5), perseptron(p_6), perseptron(p_7), perseptron(p_8)]
#+end_src

ans =

   1   1   0   0   1   0   0   1

** E4.5

\begin{equation}
\[ I) P_{1} = \begin{bmatrix}-1 \\ 1 \end{bmatrix} \implies -w_{1} + w_{2} + b  > 0 \]
\[ II) P_{3} = \begin{bmatrix} 1 \\ -1 \end{bmatrix} \implies w_{1} - w_{2} + b  > 0 \]
\[ III) P_{2} = \begin{bmatrix} -1 \\ -1 \end{bmatrix} \implies -w_{1} - w_{2} + b  < 0 \]
\[ IV) P_{4} = \begin{bmatrix} 1 \\ 1 \end{bmatrix} \implies w_{1} + w_{2} + b  < 0 \]
\[ I + II \implies b > 0 \ \& \ III + IV \implies b < 0 \]
\end{equation}

** E4.6
*** I: 
#+begin_src octave
plot(-1, 1, 'xb;Category I;', "markersize", 10);
hold on;
plot(-1, 0, 'xb', "markersize", 10, "markersize", 10);

plot(0, 2, 'or;Category II;', "markersize", 10);
plot(1, 2, 'or', "markersize", 10);

plot(2, 0, 'cd;Category III;', "markersize", 10);
plot(2, 1, 'cd', "markersize", 10);

plot(2, 0, 'mh;Category IV;', "markersize", 10);
plot(2, 1, 'mh', "markersize", 10);

x = [-3:0.01:3];
y(1:601) = 1.5;

plot(x, y , ";Y=1.5;");

x = [-3:0.01:3];
plot(x,-3*x, ";Y=-3x;")
grid on;
#+end_src
[[./graphs/e4_6_I.png]]

\begin{equation}
W= \begin{bmatrix}0 &1 \\ -3 & -1\end{bmatrix}, 
b = \begin{bmatrix}-1.5 \\ 0\end{bmatrix}
\end{equation}

*** II: 
[[./graphs/e4_6_II.png]]

*** III: 

\begin{equation}
\[hardlim(wp+b) = hardlim ( \begin{bmatrix} 0 &1 \\ -3 &-1\end{bmatrix}  \begin{bmatrix}-1 \\ -3 \end{bmatrix} + \begin{bmatrix}-1.5 \\ 0\end{bmatrix}) = \begin{bmatrix}0 \\ 1\end{bmatrix} \]
\[ e =  \begin{bmatrix} 0 \\ 1 \end{bmatrix} - \begin{bmatrix}0 \\ 1 \end{bmatrix} =  \begin{bmatrix}0 \\ 0 \end{bmatrix} \]
\[ W(1) = W(0) ,\ b(1) = b(0)
\end{equation}
[[./graphs/e4_6_II.png]]

** E4.7
*** I: 
#+begin_src octave
plot(0, 0, 'xb;Category I;', "markersize", 10);
hold on;
plot(-1, 0, 'xb', "markersize", 10, "markersize", 10);
plot(0, 1, 'xb', "markersize", 10, "markersize", 10);


plot(-1, 1, 'or;Category II;', "markersize", 10);
plot(0, 2, 'or', "markersize", 10);
plot(-2, 0, 'or', "markersize", 10);

x = [-2:0.01:2];
plot(x,x+1.5, ";y = x + 1.5;")
grid on;
#+end_src

[[./graphs/e4_7_I.png]] 
\begin{equation}
\[w =  \begin{bmatrix} 1 &-1 \end{bmatrix},\ b=1.5\]
\end{equation}

*** II: 
[[./graphs/e4_7_II.png]]

*** III: 
[[./graphs/e4_7_I.png]] 

*** IV: 
\begin{equation}
\[w =  hardlim(\begin{bmatrix} 1 &-1 \end{bmatrix} \begin{bmatrix} -3 \\ 0 \end{bmatrix} + 1.5) = 0 \Category II\]
\end{equation}

*** V: 
#+begin_src octave
plot(0, 0, 'xb;Category I;', "markersize", 10);
hold on;
plot(-1, 0, 'xb', "markersize", 10, "markersize", 10);
plot(0, 1, 'xb', "markersize", 10, "markersize", 10);


plot(-1, 1, 'or;Category II;', "markersize", 10);
plot(0, 2, 'or', "markersize", 10);
plot(-2, 0, 'or', "markersize", 10);

plot(-3, 0, 'hm;NewPoint;', "markersize", 10);

x = [-3:0.01:3];
plot(x,x+1.5, ";y = x + 1.5;")
grid on;
#+end_src
[[./graphs/e4_7_V.png]] \\
No, there is no straight line that can divide space in that order.

** E4.8
*** I: 

#+begin_src octave
plot(-1, -1, 'x;P1;', "markersize", 10);
hold on;
plot(0, 0, 'o;P2;', "markersize", 10, "markersize", 10);
plot(-1, 1, 'h;P3;', "markersize", 10);


y = [-2:0.01:2];
x(1:401) = -0.5;

plot(x, y , ";x=-0.5;");
grid on;
#+end_src
[[./graphs/e4_8_I.png]] \\
only  P1 is correctly classified.
*** II: 
\begin{equation}
\[W(0)\begin{bmatrix} 1 &0 \end{bmatrix},\ b(0)=0.5\]
\[ harlim(\begin{bmatrix} 1 &0 \end{bmatrix} \begin{bmatrix} -1 \\ -1 \end{bmatrix} + 0.5) = 0 \emplies e = 0 - 0 = 0 W(1) = W(0),\ B(1) = B(0)\]
\[ harlim(\begin{bmatrix} 1 &0 \end{bmatrix} \begin{bmatrix} 0 \\ 0 \end{bmatrix} + 0.5) = 1 \emplies e = 0 - 1 = -1 \emplies \]
\[W(2) = w+e*p^{T} = \begin{bmatrix} 1 &0 \end{bmatrix} , \ B(2) = B(1) - 1 = -0.5\]
\[ harlim(\begin{bmatrix} 1 &0 \end{bmatrix} \begin{bmatrix} -1 \\ 1 \end{bmatrix} - 0.5) = 0 \emplies e = 1 - 0 = 0  \emplies \]
\[W(3) = w+e*p^{T} = \begin{bmatrix} 0 & 1 \end{bmatrix} , \ B(3) = B(2) +1 = 0.5\]
\end{equation}

*** III: 

#+begin_src octave
plot(-1, -1, 'x;P1;', "markersize", 10);
hold on;
plot(0, 0, 'o;P2;', "markersize", 10, "markersize", 10);
plot(-1, 1, 'h;P3;', "markersize", 10);


x = [-2:0.01:2];
y(1:401) = -0.5;

plot(x, y , ";y=-0.5;");
grid on;
#+end_src
[[./graphs/e4_8_II.png]] \\
P1 and P3 are correctly classified.

*** IV: 
There is a W and a b that divide the space into our desires so by Proof of Convergence (4-15) given enough iterations the perceptron, learning rule will be successfull.
[#ASK] [#TODO: #DOTHEMATH]

** E4.9
*** I: 
#+begin_src octave
plot(1, 0, 'x;P1;', "markersize", 10);
hold on;
plot(-1, 2, 'o;P2;', "markersize", 10, "markersize", 10);
plot(1, 2, 'h;P3;', "markersize", 10);


x = [-2:0.01:2];
y(1:401) = -1;

plot(x, y , "r;y=-1;", "markersize", 10);
grid on;
#+end_src
[[./graphs/e4_9_I.png]] \\

\begin{equation}
hardlim(W*p1+b) = hardlim(W*p2+b) = hardlim(W*p3+b) = 1
\end{equation}
only p_{3} classifid correctly.

*** II: 
\begin{equation}
\[hardlim(W*p_{1}+b) = hardlim(0+1) =  1 \implies e = 0 -1 \implies \]
\[W(1) = W(0)-ep_{1}^{T} = \begin{bmatrix} 0 &1 \end{bmatrix} - \begin{bmatrix} 1 &0 \end{bmatrix} = \begin{bmatrix} 1 &-1 \end{bmatrix} \]
\[b(1) = b(0) - 1 = 0\]
\end{equation}

*** III: 
#+begin_src octave
plot(1, 0, 'x;P1;', "markersize", 10);
hold on;
plot(-1, 2, 'o;P2;', "markersize", 10, "markersize", 10);
plot(1, 2, 'h;P3;', "markersize", 10);


x = [-2:0.01:2];
y = [-2:0.01:2];

plot(x, y , "r;x=y;", "markersize", 10);
grid on;
#+end_src
[[./graphs/e4_9_III.png]] \\

\begin{equation}
\[hardlim(W*p_{1}+b) = 1 \]
\[hardlim(W*p_{2}+b) = 1 \]
\[hardlim(W*p_{1}+b) = 0 \]
\[W(1) = W(0)-ep_{1}^{T} = \begin{bmatrix} 0 &1 \end{bmatrix} - \begin{bmatrix} 1 &0 \end{bmatrix} = \begin{bmatrix} 1 &-1 \end{bmatrix} \]
\[b(1) = b(0) - 1 = 0\]
\end{equation}
*** IV:
\begin{equation}
\[hardlim(W*p_{2}+b) = hardlim(3) =  1 \implies e = 0 -1 \implies   \]
\[W(2) = W(1)-ep_{1}^{T} = \begin{bmatrix} 0 &1 \end{bmatrix} - \begin{bmatrix} 1 &-2 \end{bmatrix} = \begin{bmatrix} -1 &2 \end{bmatrix} \]
\[b(2) = b(1) -1 = -1\]
\end{equation}
*** V: 
#+begin_src octave
plot(1, 0, 'x;P1;', "markersize", 10);
hold on;
plot(-1, 2, 'o;P2;', "markersize", 10, "markersize", 10);
plot(1, 2, 'h;P3;', "markersize", 10);


x = [-2:0.01:2];

plot(x, (x+1)/2 , "r;Y = (x+1)/2;", "markersize", 10);
grid on;
#+end_src
[[./graphs/e4_9_V.png]] \\

\begin{equation}
\[hardlim(W*p_{1}+b) = hardlim(-1-1) = 0 \]
\[hardlim(W*p_{2}+b) = hardlim(5-1) = 1 \]
\[hardlim(W*p_{1}+b) = hardlim(3-1) = 1 \]
\end{equation}

*** VI: 
There is a W and b that divide the space into our desires so by Proof of Convergence (4-15) given enough iterations the perceptron, learning rule will be successfull.

** E4.10
*** I: 
 hardlim(x) = hardlim(hardlims(x)+1)
*** II, III: 
if W*p+b < 0 and t = 1:\\
hardlim neuron ans would be : 0\\
e = 1 - 0\\
W^{new} = W^old+p\\
hardlims neuron ans would be : -1\\
e = 1 - (-1) = 2 \\
W^{new} = W^old+2p\\

if W*p < 0 and t = 0:\\
hardlim neuron ans would be : 0\\
e = 0 - 0\\
W^{new} = W^old\\
hardlims neuron ans would be : -1\\
e = 0 - (-1) = 1\\
W^{new} = W^old+p\\


if Wp+b > 0 and t = 0:\\
hardlim neuron ans would be : 1\\
e = 0 - 1 = -1\\
W^{new} = W^old-p\\
hardlims neuron ans would be : 1\\
e = 0 - 1 = -1\\
W^{new} = W^old-p\\

if Wp+b > 0 and t = 1:\\
hardlim neuron ans would be : 1\\
e = 1 - 1 = 0\\
W^{new} = W^old\\
hardlims neuron ans would be : 1\\
e = 1 - 1 = 0\\
W^{new} = W^old\\


*** IV: 
[#ask] [#think]

** E4.11
*** I: 
#+begin_src octave
function a = hardlim(n)
a = n;
a(n<0) = 0;
a(n>0) = 1;
endfunction

Points = [[1,4]; [1,5]; [2,4]; [2,5]; [3,1]; [3,2]; [4,1]; [4,2];];
Targets = [0; 0; 0; 0; 1; 1; 1; 1];

function [error, weight_new, bias_new] = new_w(weight_old, bias_old, point, target) 
ans = hardlim((weight_old*point)+bias_old);
error = target - ans;
weight_new = weight_old + error * (point');
bias_new = bias_old + error;
endfunction


function [weight, bias] = learn (X, Y)
w = [0, 0];
b = 0;

not_matching_counter = 1;

while (not_matching_counter != 0)
not_matching_counter = 0;
for i = 1:rows(X)
[e,wn,bn] = new_w(w, b, X(i,:)', Y(i));
if (e != 0)
not_matching_counter += 1;
endif
w = wn;
b = bn;
endfor

endwhile

disp ("w = "), disp(w), disp("b = "), disp(b)
weight = w;
bias = b;
endfunction

learn(Points, Targets)

#+end_src
*** II: 
#+begin_src octave
function a = hardlim(n)
a = n;
a(n<0) = 0;
a(n>0) = 1;
endfunction

Points = [[1,4]; [1,5]; [2,4]; [2,5]; [3,1]; [3,2]; [4,1]; [4,2];];
Targets = [0; 0; 0; 0; 1; 1; 1; 1];

function test (X, Y, w, b )

for i = 1:rows(X)

t = hardlim(w*X(i,:)'+b);
if (t == Y(i))
disp ("matched.");
else 
disp ("not matched.")
endif

endfor

endfunction

test(Points, Targets, [4, -5], 0)
#+end_src
*** III: 
[#ASK] \\
for all points as p\\
if target of p == 1 \\
add new buffer point =  p - w \\
if target of p == 0 \\
add new buffer point  = p + w \\

** E4.12

*** I: 
#+begin_src octave
plot(1, 1, 'xb;Category I;', "markersize", 10);
hold on;
plot(1, 2, 'xb', "markersize", 10, "markersize", 10);

plot(2, 2, 'or;Category II;', "markersize", 10);
plot(2, 0, 'or', "markersize", 10);

plot(-1, 2, 'cd;Category III;', "markersize", 10);
plot(-2, 1, 'cd', "markersize", 10);

plot(-1, -1, 'mh;Category IV;', "markersize", 10);
plot(-2, -2, 'mh', "markersize", 10);

x = [-3:0.01:3];
plot(x, x+2 , ";Y=x+2;");


y = [-3:0.01:3];
x(1:601) = 1.5;
plot(x,y, ";x = 1.5;")
grid on;
#+end_src
[[./graphs/e4_12_I.png]]

*** II: 

#+begin_src octave
function a = hardlim(n)
a = n;
a(n<0) = 0;
a(n>0) = 1;
endfunction

Points = [[1,1]; [1,2]; [2,2]; [2,0]; [-1,2]; [-2,1]; [-1,-1]; [-2,-2];];
Targets = [[0, 0]; [0, 0]; [0, 1]; [0,1]; [1,0]; [1,0]; [1,1]; [1,1]];

function [error, weight_new, bias_new] = new_w(weight_old, bias_old, point, target) 
ans = hardlim((weight_old*point)+bias_old);
error = target - ans;
weight_new = weight_old + error * (point');
bias_new = bias_old + error;
endfunction


function [weight, bias] = learn (X, Y)
w = [1, 0; 0,1];
b = [1;1];

not_matching_counter = 1;

while (not_matching_counter != 0)
not_matching_counter = 0;
for i = 1:rows(X)
[e,wn,bn] = new_w(w, b, X(i,:)', Y(i,:)');
if (any(e != [0;0]))
not_matching_counter += 1;
endif
w = wn;
b = bn;
endfor

endwhile

disp ("w = "), disp(w), disp("b = "), disp(b)
weight = w;
bias = b;
endfunction

learn(Points, Targets)

#+end_src

\begin{equation}
\[ w = \begin{bmatrix}-2 &0 \\ 1 & -3 \end{bmatrix}  \]
\[ b = \begin{bmatrix}-1 \\ 0 \end{bmatrix}  \]
\end{equation}

*** III: 
#+begin_src octave
plot(1, 1, 'xb;Category I;', "markersize", 10);
hold on;
plot(1, 2, 'xb', "markersize", 10, "markersize", 10);

plot(2, 1.5, 'or;Category II;', "markersize", 10);
plot(2, 0, 'or', "markersize", 10);

plot(-1, 2, 'cd;Category III;', "markersize", 10);
plot(-2, 1, 'cd', "markersize", 10);

plot(-1, -1, 'mh;Category IV;', "markersize", 10);
plot(-2, -2, 'mh', "markersize", 10);

x = [-3:0.01:3];
plot(x, x+2 , ";Y=x+2;");


y = [-3:0.01:3];
x(1:601) = 1.5;
plot(x,y, ";x = 1.5;")
grid on;
#+end_src
[[./graphs/e4_12_III.png]]


*** IV: 

#+begin_src octave
function a = hardlim(n)
a = n;
a(n<0) = 0;
a(n>0) = 1;
endfunction

Points = [[1,1]; [1,2]; [2,1.5]; [2,0]; [-1,2]; [-2,1]; [-1,-1]; [-2,-2];];
Targets = [[0, 0]; [0, 0]; [0, 1]; [0,1]; [1,0]; [1,0]; [1,1]; [1,1]];

function [error, weight_new, bias_new] = new_w(weight_old, bias_old, point, target) 
ans = hardlim((weight_old*point)+bias_old);
error = target - ans;
weight_new = weight_old + error * (point');
bias_new = bias_old + error;
endfunction


function [weight, bias] = learn (X, Y)
w = [1, 0; 0,1];
b = [1;1];

not_matching_counter = 1;

while (not_matching_counter != 0)
not_matching_counter = 0;
for i = 1:rows(X)
[e,wn,bn] = new_w(w, b, X(i,:)', Y(i,:)');
if (any(e != [0;0]))
not_matching_counter += 1;
endif
w = wn;
b = bn;
endfor

endwhile

disp ("w = "), disp(w), disp("b = "), disp(b)
weight = w;
bias = b;
endfunction

learn(Points, Targets)

#+end_src

\begin{equation}
\[ w = \begin{bmatrix}-2 &0 \\ 3 & -5 \end{bmatrix}  \]
\[ b = \begin{bmatrix}0 \\ 2 \end{bmatrix}  \]
\end{equation}
** E4.13
[#ASK][#TODO][#THINK]
* Chapter 5
** E5.1
Let p_{1} and p_{2} be two vectors on the decision boundary:
\begin{equation}
\[Wp_{1} + b = 0 , \ Wp_{2} + b = 0\] 
\[Wp_{1}+b+Wp_{2}+b = 0 \implies \[W(p_{1}+p_{2})+b = -b \]
\end{equation}
therefore the sum is not on the decision boundary and condition 1 is not satisfied.
** E5.2
1
\begin{equation}
\[Wp = 0 \implies\begin{bmatrix}w_{1} &w_{2}  &....  &w_{R} \end{bmatrix} \begin{bmatrix}p_{1} \\p_{2}  \\....  \\p_{R}\end{bmatrix}  = 0\]
\[p = \text{a linear combination of normalized perpendicular vectors To W}\]
\end{equation}
dimention of the vector space is R-1.
** E5.3
\begin{equation}
\[f(x),\ g(x) \in S \ |\ f(0) = g(0) = 0\] 
\[1) f(0) + g(0) = 0  \implies f(x)+g(x) \in S\]
\[2) f(x)+g(y) = g(y)+f(x)\]
\[3) (f(x)+g(y))+k(x) = f(x)+(g(y)+k(x))\]
\[4) (f(x) + 0 = f(x)\]
\[5) x = f(y),\ -f(0) =0 \implies -x = -f(y) \]
\[6) (a*f(0) = a*0 = 0 \implies a*f(x) \in S\]
\[7) 1\times f(x) = f(x) \]
\[8) a*b*f(0) = 0 \]
\[9) (a+b) f(x) = af(x) + bf(x) \]
\[10) a (f(x)+g(x)) = af(x) + ag(x) \]
\end{equation}
** E5.4

\begin{equation}
\[ \begin{bmatrix}a &b\\  c &d\end{bmatrix}  \in S\]
\[ 1) \begin{bmatrix}a &b\\  c &d\end{bmatrix} \begin{bmatrix}a' &b'\\  c' &d'\end{bmatrix} = \begin{bmatrix}a+a' &b+b'\\  c+c' &d+d'\end{bmatrix}\]
\[2) \begin{bmatrix}a &b\\  c &d\end{bmatrix} + \begin{bmatrix}a' &b'\\  c' &d'\end{bmatrix} = \begin{bmatrix}a' &b'\\  c' &d'\end{bmatrix} + \begin{bmatrix}a &b\\  c &d\end{bmatrix} = \begin{bmatrix}a+a' &b+b'\\  c+c' &d+d'\end{bmatrix}\]
\[ 3) (\begin{bmatrix}a_{x} &b_{x}\\  c_{x} &d_{x}\end{bmatrix} + \begin{bmatrix}a_{y} &b_{y}\\  c_{y} &d_{y}\end{bmatrix}) + \begin{bmatrix}a_{z} &b_{z}\\  c_{z} &d_{z}\end{bmatrix} = \begin{bmatrix}a_{x} &b_{x}\\  c_{x} &d_{x}\end{bmatrix} + ( \begin{bmatrix}a_{y} &b_{y}\\  c_{y} &d_{y}\end{bmatrix} + \begin{bmatrix}a_{z} &b_{z}\\  c_{z} &d_{z}\end{bmatrix}) =  \begin{bmatrix}a_{x}+a_{y}+a_{z} &b_{x}+b_{y}+b_{z}\\  c_{x}+c_{y}+c_{z} &d_{x}+d_{y}+d_{z}\end{bmatrix}\]
\[ 4) \begin{bmatrix}a &b\\  c &d\end{bmatrix} + \begin{bmatrix}0 &0\\  0 &0\end{bmatrix} = \begin{bmatrix}a &b\\  c &d\end{bmatrix}\]
\[ 5) \begin{bmatrix}a &b\\  c &d\end{bmatrix} + \begin{bmatrix}-a &-b\\  -c &-d\end{bmatrix} = \begin{bmatrix}0 &0\\  0 &0\end{bmatrix}\]
\[ 6) q \begin{bmatrix}a &b\\  c &d\end{bmatrix} = \begin{bmatrix}qa &qb\\  qc &qd\end{bmatrix}\]
\[ 7) 1 \begin{bmatrix}a &b\\  c &d\end{bmatrix} = \begin{bmatrix}a &b\\  c &d\end{bmatrix} \]
\[ 8) p(q\begin{bmatrix}a &b\\  c &d\end{bmatrix}) = (pq) \begin{bmatrix}a &b\\  c &d\end{bmatrix} = \begin{bmatrix}pqa &pqb\\  pqc &pqd\end{bmatrix} \]
\[ 9) (p+q) \begin{bmatrix}a &b\\  c &d\end{bmatrix} = a \begin{bmatrix}a &b\\  c &d\end{bmatrix} + b\begin{bmatrix}a &b\\  c &d\end{bmatrix} = \begin{bmatrix}(p+q)a &(p+q)b\\  (p+q)c &(p+q)d\end{bmatrix}\]
\[ 10) q (\begin{bmatrix}a_{x} &b_{x}\\  c_{x} &d_{x}\end{bmatrix} +  \begin{bmatrix}a_{y} &b_{y}\\  c_{y} &d_{y}\end{bmatrix}) = q\begin{bmatrix}a_{x} &b_{x}\\  c_{x} &d_{x}\end{bmatrix} + q \begin{bmatrix}a_{y} &b_{y}\\  c_{y} &d_{y}\end{bmatrix}  = \begin{bmatrix}q(a_{x}+a_{y}) &q(b_{x}+b_{y})\\  q(c_{x}+c_{y}) &q(d_{x}+d_{y})\end{bmatrix}\]
\end{equation}

** E5.5
*** I: 
\begin{equation}
WP + b = 0 \implies WP = 0
\end{equation}

*** II:  
same as P5.1:
*** III: 
same as E5.2 = 3-1 =2;
*** IV :
\begin{equation}
\[ P = \begin{bmatrix} 2, 4, 2 \end{bmatrix}^T\]
\[W.X = 0 \implies 1*x_1 + 0+x_2 -1x_3 = 0 \implies x1_1 = 1 , x_2 = 0, x_3 = 1 \implies v_1 = \begin{bmatrix} \frac{\sqrt{2}}{2}, 0, \frac{\sqrt{2}}{2}} \end{bmatrix}^T  \]
\[ v_2 = [0,1,0]\]
\end{equation}

** E5.6
*** I: 
not a vector space
\begin{equation}
\[f(x) \in S \iff f(0.5) = 2:\]
\[ f(x) \in S\ and\ g(x) \in S \implies f(0.5)+g(0.5) = 2 +2 = 4 \implies f(x)+g(x) \notin S\]
\end{equation}

*** II: 
\begin{equation}
\[f(x) \in S \iff f(0.75) = 0:\]
\[ f(x) \in S \implies f(ax) = af(x) \implies f(a*0.75) = a \times 0 = 0\text{ not always true } \implies f(x)\notin S\]
\end{equation}

*** III: 
not a vector space becase we can't find a 0 \in S that satisfy condition 4
\begin{equation}
\[0 \in s \iff 0(0.5) = -0(0.75)-3 \]
\[f(0.5)+0(0.50) = -f(0.75) -3 + 0(0.75) -3 \implies f(0.5) = -f(0.75)-6\]
\end{equation}
** E5.7
*** I: 
It's a vector space.
*** II: 
Not a vector space 
\begin{equation}
\[X + (-X) = 0\]
\[X(t>0) > 0  \implies -X(t>0) < 0 \implies -X \notin S \]
\end{equation}
*** III: 
[#ASK]
It's a vector space.
** E5.8
*** I: 
\begin{equation}
\[a_{1}\begin{bmatrix} 1 \\ 2 \\3\end{bmatrix}  + a_{2} \begin{bmatrix} 1 \\ 0 \\1\end{bmatrix} + a_{3}\begin{bmatrix} 1 \\ 2 \\1\end{bmatrix} = \begin{bmatrix} 0 \\ 0 \\0\end{bmatrix}\]
\[a_{1} + a_{2} + a_{3} = 0 \]
\[2a_{1} + 2a_{3} = 0 \]
\[3a_{1} + a_{2} + a_{3} = 0 \]
\[a_{1} = a_{2} = a_{3} = 0
\end{equation}

#+begin_src octave
A = [1, 1 ,1; 2,0,2; 3, 1 ,1]
rank(A)
#+end_src
ans = 3
*** II: 
\begin{equation}
\[cos(2t) = cos^2{t} - sin^2{t}\]
\end{equation}
#+begin_src octave
pkg load symbolic
sym t
A = [sin(t), cos(t), cos(2t)]
rank(A)
#+end_src
ans = 1
*** III: 
\begin{equation}
\[ (1+t) = -(1-t) + 2 \]
\end{equation}
#+begin_src octave
pkg load symbolic
sym t
A = [1+t, 1-t]
rank(A)
#+end_src
ans = 1

*** IV: 

#+begin_src octave
A = [1, 1 ,3; 2,0,4; 2, 0 ,4; 1,1,3]
rank(A)
#+end_src
ans = 2

\begin{equation}
\[a_{1}\begin{bmatrix} 1 \\ 2 \\2 \\1\end{bmatrix}  + a_{2} \begin{bmatrix} 1 \\ 0 \\0 \\1\end{bmatrix} + a_{3}\begin{bmatrix} 3 \\4 \\4 \\3\end{bmatrix} = \begin{bmatrix} 0 \\ 0 \\0 \\0\end{bmatrix}\]
\[a_{1} + a_{2} + 3a_{3} = 0 \]
\[2a_{1} + 4a_{3} = 0 \]
\[a_{1} + a_{2} + 3a_{3} = 0 \]
\[\implies\]
\[a_{1} = -2a_{3}\]
\[a_{2} = -a_{3}\]
\end{equation}

\begin{equation}
\[\begin{bmatrix} 1 \\ 0 \\0 \\1\end{bmatrix}, \begin{bmatrix} 0 \\ 1 \\1 \\0\end{bmatrix}\]
\end{equation}

** E5.9
\theta is angle between p_1 nad p_2

\begin{equation}
\[\cos{\theta} = \frac{(p_{1}.p_{2})}{||\norm{p_{1}}||\ ||\norm{p_{2}}||} = \frac{1}{3} \implies \theta = 70.52 degrees\]
\[\cos{\theta} = \frac{(p_{1}.p)}{||\norm{p_{1}}||\ ||\norm{p}||} = \frac{1}{3} \implies \theta = 70.52 degrees\]
\[\cos{\theta} = \frac{(p_{2}.p)}{||\norm{p_{2}}||\ ||\norm{p}||} = \frac{-1}{3} \implies \theta = 109.47 degrees\]
\end{equation}
** E5.10
#+begin_src octave
y1 =[1;0;0];
y2 = [1;1;0];
y3 = [1;1;1];
Y = [y1,y2,y3];

if (rank (Y) < 3)
disp ("Fail")
exit
endif

V = [];
c = columns(Y);

for (i = 1:c)

if (i == 1)
V = Y(:,1);
else 
s = [0;0;0];


for (j = 1:i-1)
s += ( (V(:,j)'*Y(:,i)) / (V(:,j)'*V(:,j)) ) * V(:,j);
endfor

V = [V, Y(:,i)- s];
endif 

endfor
disp(V)
#+end_src


** E5.11
*** I:
\begin{equation}
\[a_{1}f_{1}(t)  + a_{2} f_{2}(2) + a_{3} f_{3}(t) = 0\]
\[ [0, \frac{1}{4}] \implies a_{1}  + a_{2} + a_{3}  = 0\]
\[ [\frac{1}{4}, \frac{3}{4}] \implies a_{1}  - a_{2} + a_{3}  = 0\]
\[ [\frac{3}{4}, 1] \implies a_{1}  - a_{2} - a_{3}  = 0\]
\[ \implies a_{1} = a_{2} = a_{3} = 0 \]
\end{equation}
*** II:
\begin{equation}
\[v_{1} = f_{1}(t)\]
\[ v_{2}  = f_{2} - \frac{ \int_{0}^{1} v_{1}  f_{2} } { \int_{0}^{1} v_{1}  v_{1} } f_1 = f_{2} - \frac{ \int_{0}^{\frac{1}{4}} f_{1} f_{2} + \int_{\frac{1}{4}}^{1} f_{1} f_{2} } { 1 } f_1= f_{2} - (\frac{1}{4} - \frac{3}{4})f_1 = f_{2}+\frac{1}{2}f_1 \]
\[ v_{3} = f_{3} - (\frac{\int_{0}^{1} v_{1}  f_{3}} {\int_{0}^{1} v_{1}  v_{1}}v_1 + \frac{\int_{0}^{1} v_{2}  f_{3}} {\int_{0}^{1} v_{2}  v_{2}}v_2) = f_{3} - (\frac{1}{2} f_1 + \frac{0}{\frac{-3}{4}}v_2) = f_{3} - \frac{1}{2} f_1\]
\end{equation}
** E5.12
*** I: 
\begin{equation}
\[v_{1} = f_{1}(t)\]
\[ v_{2}  = f_{2} - \frac{ \int_{0}^{1} v_{1}  f_{2} } { \int_{0}^{1} v_{1}  v_{1} }f_1  = f_{2} - \frac{ \int_{0}^{\frac{1}{4}} f_{1} f_{2} + \int_{\frac{1}{4}}^{\frac{3}{4}} f_{1} f_{2} \int_{\frac{3}{4}}^{1} f_{1} f_{2}} { 1 } f_1= f_{2} - \frac{\frac{1}{2}}{1} f_1= f_{2} - \frac{1}{2}f1\]
\end{equation}

*** II: 
**** g(X)

\begin{equation}
\text{at 0}
\[af_1(0)+b(f_2(0) - \frac{1}{2}f_1(0)  = 3 \]
\[ -a + \frac{1}{2} b = 3\]
\[\text{at } \frac{3}{4}\]
\[af_1(\frac{3}{4})+b(f_2(\frac{3}{4}) - \frac{1}{2}f_1(\frac{3}{4})  = -1 \]
\[ a + \frac{3}{2} b = -1\]
\[\implies a = 4, b = 2\]
\end{equation}
